{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a6284db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### What does a correlation matrix tell us? How do we interpret a heatmap of correlations?\n",
    "\n",
    " What a Correlation Matrix Tells Us:\n",
    "Shows pairwise linear relationships between variables.\n",
    "\n",
    "Values range from -1 to +1:\n",
    "\n",
    "+1: Perfect positive correlation (both increase together).\n",
    "\n",
    "-1: Perfect negative correlation (one increases, other decreases).\n",
    "\n",
    "0: No linear correlation.\n",
    "\n",
    " Interpreting a Correlation Heatmap:\n",
    "Colors represent strength and direction of correlation.\n",
    "\n",
    "Darker/warmer colors (e.g., red) = strong positive correlation.\n",
    "\n",
    "Cooler colors (e.g., blue) = strong negative correlation.\n",
    "\n",
    "Near-neutral colors = little or no correlation.\n",
    "\n",
    "Helps quickly identify related features or multicollinearity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab0d8b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Why is it important to understand which features are highly correlated before building\n",
    "a predictive model?\n",
    "\n",
    "Avoid Multicollinearity\n",
    "\n",
    "Highly correlated features can cause redundant information, leading to unstable or unreliable model estimates (especially in linear regression).\n",
    "\n",
    "Simplify the Model\n",
    "\n",
    "Removing or combining correlated features reduces complexity without losing much information.\n",
    "\n",
    "Improve Model Performance\n",
    "\n",
    "Reducing correlated features can help models generalize better and avoid overfitting.\n",
    "\n",
    "Better Interpretability\n",
    "\n",
    "Models with independent features are easier to understand and explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee0e1b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### How can multicollinearity between variables affect model performance and\n",
    "interpretability?\n",
    "\n",
    "\n",
    " Effects of Multicollinearity on Model Performance and Interpretability\n",
    "Unstable Coefficients\n",
    "\n",
    "Makes coefficient estimates highly sensitive to small data changes.\n",
    "\n",
    "Leads to large standard errors and unreliable p-values.\n",
    "\n",
    "Difficulty in Interpretation\n",
    "\n",
    "Hard to tell the individual effect of correlated variables.\n",
    "\n",
    "Coefficients can have wrong signs or magnitudes.\n",
    "\n",
    "Reduced Model Predictive Power\n",
    "\n",
    "May inflate variance and reduce accuracy on new data.\n",
    "\n",
    "Some models (like linear regression) perform poorly with multicollinearity.\n",
    "\n",
    "Overfitting Risk\n",
    "\n",
    "Model may fit noise instead of true signal.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
